{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":6466836,"sourceType":"datasetVersion","datasetId":3734319},{"sourceId":10321941,"sourceType":"datasetVersion","datasetId":6390805}],"dockerImageVersionId":30822,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, Dataset\nfrom torchvision.transforms import Compose, ToTensor\nfrom PIL import Image\nimport matplotlib.pyplot as plt\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-04T12:33:42.497337Z","iopub.execute_input":"2025-01-04T12:33:42.497672Z","iopub.status.idle":"2025-01-04T12:33:48.152200Z","shell.execute_reply.started":"2025-01-04T12:33:42.497640Z","shell.execute_reply":"2025-01-04T12:33:48.150979Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"# Hyperparameters\nepochs = 200\nbatch_size = 50\nlatent_dim = 25\nh_image, w_image = 96,96 # Target image size for training\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-04T12:33:48.153172Z","iopub.execute_input":"2025-01-04T12:33:48.153810Z","iopub.status.idle":"2025-01-04T12:33:48.163079Z","shell.execute_reply.started":"2025-01-04T12:33:48.153762Z","shell.execute_reply":"2025-01-04T12:33:48.161154Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"#vae model\nclass VAE(nn.Module):\n    def __init__(self, latent_dim):\n        super(VAE, self).__init__()\n        # Encoder\n        self.encoder = nn.Sequential(\n            nn.Conv2d(1, 16, kernel_size=5, stride=2, padding=2),\n            nn.ReLU(),\n            nn.Conv2d(16, 32, kernel_size=3, stride=2, padding=1),\n            nn.ReLU(),\n            nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1),\n            nn.ReLU(),\n            nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1),\n            nn.ReLU()\n        )\n        self.flatten = nn.Flatten()\n        self.fc_mean = nn.Linear(128 * 6 * 6, latent_dim)  # Update input size here\n        self.fc_var = nn.Linear(128 * 6 * 6, latent_dim)\n        self.fc_decoder = nn.Linear(latent_dim, 128 * 6 * 6)\n        self.decoder = nn.Sequential(\n            nn.ConvTranspose2d(128, 128, kernel_size=3, stride=2, padding=1, output_padding=1),\n            nn.ReLU(),\n            nn.ConvTranspose2d(128, 64, kernel_size=3, stride=2, padding=1, output_padding=1),\n            nn.ReLU(),\n            nn.ConvTranspose2d(64, 32, kernel_size=3, stride=2, padding=1, output_padding=1),\n            nn.ReLU(),\n            nn.ConvTranspose2d(32, 16, kernel_size=5, stride=2, padding=2, output_padding=1),\n            nn.ReLU(),\n            nn.ConvTranspose2d(16, 1, kernel_size=3, stride=1, padding=1),\n            nn.Sigmoid()\n        )\n\n    def reparameterize(self, mean, var):\n        std = torch.exp(0.5 * var)\n        eps = torch.randn_like(std)\n        return mean + eps * std\n\n    def forward(self, x):\n        x = self.encoder(x)\n        x = self.flatten(x)\n        mean = self.fc_mean(x)\n        var = self.fc_var(x)\n        z = self.reparameterize(mean, var)\n        x = self.fc_decoder(z)\n        x = x.view(-1, 128, 6, 6)\n        x = self.decoder(x)\n        return x, mean, var","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-04T12:33:48.164468Z","iopub.execute_input":"2025-01-04T12:33:48.164798Z","iopub.status.idle":"2025-01-04T12:33:48.187553Z","shell.execute_reply.started":"2025-01-04T12:33:48.164770Z","shell.execute_reply":"2025-01-04T12:33:48.184235Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"# Loss Function\ndef vae_loss(recon_x, x, mean, var):\n    # recon_loss = nn.functional.mse_loss(recon_x, x, reduction='sum')\n    recon_loss = nn.functional.binary_cross_entropy(recon_x, x, reduction='sum')\n\n    kl_loss = -0.5 * torch.sum(1 + var - mean.pow(2) - var.exp())\n    return recon_loss + kl_loss\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-04T12:33:48.189775Z","iopub.execute_input":"2025-01-04T12:33:48.190902Z","iopub.status.idle":"2025-01-04T12:33:48.210535Z","shell.execute_reply.started":"2025-01-04T12:33:48.190853Z","shell.execute_reply":"2025-01-04T12:33:48.209006Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"# Device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n# Custom Dataset\nclass CityscapesDataset(Dataset):\n    def __init__(self, root_dir, transform=None):\n        self.root_dir = root_dir\n        self.image_files = [os.path.join(root_dir, f) for f in os.listdir(root_dir) if f.endswith('.jpg') or f.endswith('.png')]\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.image_files)\n\n    def __getitem__(self, idx):\n        img_path = self.image_files[idx]\n        image = Image.open(img_path).convert('L')  \n        if self.transform:\n            image = self.transform(image)\n        return image","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-04T12:33:48.212719Z","iopub.execute_input":"2025-01-04T12:33:48.213268Z","iopub.status.idle":"2025-01-04T12:33:48.241014Z","shell.execute_reply.started":"2025-01-04T12:33:48.213204Z","shell.execute_reply":"2025-01-04T12:33:48.239776Z"}},"outputs":[{"name":"stdout","text":"Using device: cpu\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"# Dataset Paths\ndataset_dir = '/kaggle/input/cityscapes/train/img'  ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-04T12:33:48.242323Z","iopub.execute_input":"2025-01-04T12:33:48.242700Z","iopub.status.idle":"2025-01-04T12:33:48.259757Z","shell.execute_reply.started":"2025-01-04T12:33:48.242663Z","shell.execute_reply":"2025-01-04T12:33:48.258308Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"# Transformations for left part\nclass LeftCrop:\n    def __init__(self, size):\n        self.size = size\n\n    def __call__(self, image):\n        return image.crop((0, 0, self.size[0], self.size[1]))\n\ntransform_left = Compose([\n    LeftCrop((96, 96)),\n    ToTensor(),\n])\n# Load datasets for left and right images\ndataset_left = CityscapesDataset(root_dir=dataset_dir, transform=transform_left)\n# Dataloaders\ndataloader_left = DataLoader(dataset_left, batch_size=batch_size, shuffle=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-04T12:33:48.261441Z","iopub.execute_input":"2025-01-04T12:33:48.262342Z","iopub.status.idle":"2025-01-04T12:33:48.481169Z","shell.execute_reply.started":"2025-01-04T12:33:48.262296Z","shell.execute_reply":"2025-01-04T12:33:48.479998Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"#  Transformations for right part\nclass RightCrop:\n    def __init__(self, size):\n        self.size = size\n\n    def __call__(self, image):\n        image_width = image.width\n        return image.crop((image_width - self.size[0], 0, image_width, self.size[1]))\n\ntransform_right = Compose([\n    RightCrop((96, 96)),\n    ToTensor(),\n])\ndataset_right = CityscapesDataset(root_dir=dataset_dir, transform=transform_right)\ndataloader_right = DataLoader(dataset_right, batch_size=batch_size, shuffle=True)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-04T12:33:48.482118Z","iopub.execute_input":"2025-01-04T12:33:48.482509Z","iopub.status.idle":"2025-01-04T12:33:48.508981Z","shell.execute_reply.started":"2025-01-04T12:33:48.482468Z","shell.execute_reply":"2025-01-04T12:33:48.507376Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"# Initialize Model and Optimizer\nvae = VAE(latent_dim).to(device)\noptimizer = optim.Adam(vae.parameters(), lr=0.001)\n# Training loop for left part images\nfor epoch in range(epochs):\n    vae.train()\n    train_loss = 0\n    for batch in dataloader_left:\n        left_image = batch.to(device)\n        optimizer.zero_grad()\n        recon_x, mean, var = vae(left_image)\n        loss = vae_loss(recon_x, left_image, mean, var)\n        loss.backward()\n        train_loss += loss.item()\n        optimizer.step()\n\n    print(f\"Epoch {epoch + 1}, Loss: {train_loss / len(dataloader_left.dataset):.4f}\")\n\n# Save the model after training\ntorch.save(vae.state_dict(), \"vae_left_model.pth\")\nprint(\"Model saved for left part\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-04T12:33:48.510340Z","iopub.execute_input":"2025-01-04T12:33:48.510766Z"}},"outputs":[{"name":"stdout","text":"Epoch 1, Loss: 5622.0710\nEpoch 2, Loss: 5418.6849\nEpoch 3, Loss: 5329.4353\nEpoch 4, Loss: 5237.2745\nEpoch 5, Loss: 5202.8994\nEpoch 6, Loss: 5191.6126\nEpoch 7, Loss: 5178.1017\nEpoch 8, Loss: 5173.0608\nEpoch 9, Loss: 5166.6909\nEpoch 10, Loss: 5162.6443\nEpoch 11, Loss: 5158.0510\nEpoch 12, Loss: 5154.1058\nEpoch 13, Loss: 5151.7131\nEpoch 14, Loss: 5150.8892\nEpoch 15, Loss: 5145.8149\nEpoch 16, Loss: 5143.5682\nEpoch 17, Loss: 5140.8395\nEpoch 18, Loss: 5138.4157\nEpoch 19, Loss: 5136.5334\nEpoch 20, Loss: 5134.0772\nEpoch 21, Loss: 5132.7660\nEpoch 22, Loss: 5130.9791\nEpoch 23, Loss: 5128.6579\nEpoch 24, Loss: 5128.7781\nEpoch 25, Loss: 5126.1630\nEpoch 26, Loss: 5128.7244\nEpoch 27, Loss: 5124.7843\nEpoch 28, Loss: 5121.9531\nEpoch 29, Loss: 5119.6676\nEpoch 30, Loss: 5118.5167\nEpoch 31, Loss: 5117.1983\nEpoch 32, Loss: 5117.8383\nEpoch 33, Loss: 5115.9203\nEpoch 34, Loss: 5114.3748\nEpoch 35, Loss: 5113.1187\nEpoch 36, Loss: 5111.2699\nEpoch 37, Loss: 5110.0986\nEpoch 38, Loss: 5111.8304\nEpoch 39, Loss: 5109.4749\nEpoch 40, Loss: 5108.9249\nEpoch 41, Loss: 5107.8083\nEpoch 42, Loss: 5106.6453\nEpoch 43, Loss: 5108.4540\nEpoch 44, Loss: 5104.4576\nEpoch 45, Loss: 5103.5482\nEpoch 46, Loss: 5103.9160\nEpoch 47, Loss: 5102.3733\nEpoch 48, Loss: 5102.2277\nEpoch 49, Loss: 5100.7163\nEpoch 50, Loss: 5101.7773\nEpoch 51, Loss: 5100.0187\nEpoch 52, Loss: 5099.0465\nEpoch 53, Loss: 5099.0167\nEpoch 54, Loss: 5097.9451\nEpoch 55, Loss: 5096.5287\nEpoch 56, Loss: 5097.2679\nEpoch 57, Loss: 5096.5518\nEpoch 58, Loss: 5095.4665\nEpoch 59, Loss: 5095.9331\nEpoch 60, Loss: 5095.3953\nEpoch 61, Loss: 5094.5374\nEpoch 62, Loss: 5093.1689\nEpoch 63, Loss: 5092.6782\nEpoch 64, Loss: 5091.9678\nEpoch 65, Loss: 5092.3792\nEpoch 66, Loss: 5090.9891\nEpoch 67, Loss: 5091.6995\nEpoch 68, Loss: 5090.5828\nEpoch 69, Loss: 5090.5321\nEpoch 70, Loss: 5090.9222\nEpoch 71, Loss: 5089.1242\nEpoch 72, Loss: 5088.2355\nEpoch 73, Loss: 5090.2557\nEpoch 74, Loss: 5088.2119\nEpoch 75, Loss: 5087.0985\nEpoch 76, Loss: 5086.4324\nEpoch 77, Loss: 5086.0523\nEpoch 78, Loss: 5086.2572\nEpoch 79, Loss: 5085.2728\nEpoch 80, Loss: 5085.5626\nEpoch 81, Loss: 5084.8373\nEpoch 82, Loss: 5084.4740\nEpoch 83, Loss: 5084.0804\nEpoch 84, Loss: 5084.0524\nEpoch 85, Loss: 5083.1449\nEpoch 86, Loss: 5082.9474\nEpoch 87, Loss: 5082.3285\nEpoch 88, Loss: 5082.4546\nEpoch 89, Loss: 5082.5810\nEpoch 90, Loss: 5081.9921\nEpoch 91, Loss: 5082.1689\nEpoch 92, Loss: 5081.9162\nEpoch 93, Loss: 5082.5425\nEpoch 94, Loss: 5080.6797\nEpoch 95, Loss: 5079.9015\nEpoch 96, Loss: 5080.0105\nEpoch 97, Loss: 5080.7315\nEpoch 98, Loss: 5081.3993\nEpoch 99, Loss: 5079.2230\nEpoch 100, Loss: 5078.9943\nEpoch 101, Loss: 5078.6253\nEpoch 102, Loss: 5078.0872\nEpoch 103, Loss: 5077.9802\nEpoch 104, Loss: 5077.6143\nEpoch 105, Loss: 5078.8883\nEpoch 106, Loss: 5078.1033\nEpoch 107, Loss: 5078.2571\nEpoch 108, Loss: 5077.4771\nEpoch 109, Loss: 5077.2817\nEpoch 110, Loss: 5076.3983\nEpoch 111, Loss: 5077.0512\nEpoch 112, Loss: 5075.8390\nEpoch 113, Loss: 5075.3988\nEpoch 114, Loss: 5075.6258\nEpoch 115, Loss: 5075.4721\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"# Initialize Model and Optimizer\nvae = VAE(latent_dim).to(device)\noptimizer = optim.Adam(vae.parameters(), lr=0.001)\n# Training loop for right part images\nfor epoch in range(epochs):\n    vae.train()\n    train_loss = 0\n    for batch in dataloader_right:\n        right_image = batch.to(device)\n        optimizer.zero_grad()\n        recon_x, mean, var = vae(right_image)\n        loss = vae_loss(recon_x, right_image, mean, var)\n        loss.backward()\n        train_loss += loss.item()\n        optimizer.step()\n\n    print(f\"Epoch {epoch + 1}, Loss: {train_loss / len(dataloader_right.dataset):.4f}\")\n\n# Save the model after training\ntorch.save(vae.state_dict(), \"vae_right_model.pth\")\nprint(\"Model saved for right part\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"vae.eval()  # Set the model to evaluation mode\nwith torch.no_grad():\n    # Fetch a small batch from the dataloader for the left-cropped images\n    sample_left = next(iter(dataloader_left)).to(device)[:2]  # First 2 images of left-cropped part\n    sample_right = next(iter(dataloader_right)).to(device)[:2]  # First 2 images of right-cropped part\n    print(f\"Input shape (Left Part): {sample_left.shape}\")  # Debug input shape for left part\n    print(f\"Input shape (Right Part): {sample_right.shape}\")  # Debug input shape for right part\n\n    # Print the image indices being processed\n    print(\"Processing images with indices: 0, 1\")  # These are the first two images from the batch\n\n    # Pass through VAE for left part\n    recon_x_left, mean_left, var_left = vae(sample_left)\n    print(f\"Reconstructed shape (Left Part): {recon_x_left.shape}\")  # Debug output shape for left part\n\n    # Pass through VAE for right part\n    recon_x_right, mean_right, var_right = vae(sample_right)\n    print(f\"Reconstructed shape (Right Part): {recon_x_right.shape}\")  # Debug output shape for right part\n\n    # Check latent space\n    print(\"Latent Mean (Left Part):\", mean_left)\n    print(\"Latent Variance (Left Part):\", var_left)\n    print(\"Latent Mean (Right Part):\", mean_right)\n    print(\"Latent Variance (Right Part):\", var_right)\n\n    # Plot results\n    plt.figure(figsize=(12, 10))\n    for i in range(len(sample_left)):\n        # Original left-cropped image\n        plt.subplot(len(sample_left), 4, 4 * i + 1)\n        plt.imshow(sample_left[i].cpu().squeeze(), cmap='gray')\n        plt.title(f\"Original Left Part (Image {i})\")\n        plt.axis('off')\n\n        # Reconstructed left-cropped image\n        plt.subplot(len(sample_left), 4, 4 * i + 2)\n        plt.imshow(recon_x_left[i].cpu().squeeze(), cmap='gray')\n        plt.title(f\"Reconstructed Left Part (Image {i})\")\n        plt.axis('off')\n\n        # Original right-cropped image\n        plt.subplot(len(sample_right), 4, 4 * i + 3)\n        plt.imshow(sample_right[i].cpu().squeeze(), cmap='gray')\n        plt.title(f\"Original Right Part (Image {i})\")\n        plt.axis('off')\n\n        # Reconstructed right-cropped image\n        plt.subplot(len(sample_right), 4, 4 * i + 4)\n        plt.imshow(recon_x_right[i].cpu().squeeze(), cmap='gray')\n        plt.title(f\"Reconstructed Right Part (Image {i})\")\n        plt.axis('off')\n\n    plt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch\nimport matplotlib.pyplot as plt\nfrom PIL import Image\nimport torchvision.transforms as transforms\n\n# Load your input image (path: '/kaggle/input/byopproject/testset/middle/train1.png')\ninput_img_path = '/kaggle/input/byopproject/testset/middle/train1.png'\ninput_img = Image.open(input_img_path).convert('L') \n\n# Resize the input image to match the height and width of the parts\ninput_img_resized = input_img.resize((recon_x_left.shape[3], recon_x_left.shape[2]))\n\n# Convert the input image to a tensor\ntransform = transforms.ToTensor()\ninput_tensor = transform(input_img_resized).unsqueeze(0).to(device)  \n\n# Replicate the input tensor to match the batch size of recon_x_left and recon_x_right\ninput_tensor = input_tensor.repeat(recon_x_left.shape[0], 1, 1, 1)\n\nvae.eval() \nwith torch.no_grad():\n    # Fetch a small batch from the dataloader for the left-cropped images\n    sample_left = next(iter(dataloader_left)).to(device)[:2]  \n    sample_right = next(iter(dataloader_right)).to(device)[:2]  \n    print(f\"Input shape (Left Part): {sample_left.shape}\") \n    print(f\"Input shape (Right Part): {sample_right.shape}\") \n\n    # Pass through VAE for left part\n    recon_x_left, mean_left, var_left = vae(sample_left)\n    print(f\"Reconstructed shape (Left Part): {recon_x_left.shape}\") \n\n    # Pass through VAE for right part\n    recon_x_right, mean_right, var_right = vae(sample_right)\n    print(f\"Reconstructed shape (Right Part): {recon_x_right.shape}\")  \n    # latent space\n    print(\"Latent Mean (Left Part):\", mean_left)\n    print(\"Latent Variance (Left Part):\", var_left)\n    print(\"Latent Mean (Right Part):\", mean_right)\n    print(\"Latent Variance (Right Part):\", var_right)\n\n   \n    stitched_image = torch.cat((recon_x_left, input_tensor, recon_x_right), dim=3) \n   \n    plt.figure(figsize=(10, 5))\n    for i in range(len(sample_left)):\n        \n        plt.subplot(len(sample_left), 2, 2 * i + 1)\n        plt.imshow(stitched_image[i].cpu().squeeze(), cmap='gray')\n        plt.title(f\"Stitched Image (Image {i})\")\n        plt.axis('off')\n\n    plt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}